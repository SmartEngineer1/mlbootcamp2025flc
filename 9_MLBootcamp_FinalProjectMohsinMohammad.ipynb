{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartEngineer1/mlbootcamp2025flc/blob/main/9_MLBootcamp_FinalProjectMohsinMohammad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Bootcamp 2025\n",
        "\n",
        "### Final Project: Train a Deep Learning model to identify Grocery item"
      ],
      "metadata": {
        "id": "DoJf9K5qna65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive #for the mounting\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split #for actual splitting to be done\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import csv\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "W9Dh8fuk22i9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/', force_remount=True)\n",
        "!ls /content/drive/MyDrive/Datasets/\n",
        "!ls /content/drive/MyDrive/Datasets/GroceryStoreDataset/"
      ],
      "metadata": {
        "id": "B3JuyWDdGX7B",
        "outputId": "61041ecb-6873-4f4a-a690-f4992d3fdd9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "GroceryStoreDataset\n",
            "GroceryStoreDataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Create directory 'Datasets/GroceryStoreDataset', unzip the shared dataset in it and mount the Google Drive\n",
        "# The original dataset used is: https://www.kaggle.com/datasets/validmodel/grocery-store-dataset?resource=download and it has been reduced further for our use-case\n",
        "data_dir = '/content/drive/MyDrive/Datasets/GroceryStoreDataset'  # Mount dataset in Google Drive"
      ],
      "metadata": {
        "id": "i9aVO01L27cu"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "DiJakegpcKuW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9f0e73-e660-4cfb-be9d-b0b25d117701"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_class_names(csv_file_path):\n",
        "    \"\"\"\n",
        "    Extracts class names from a CSV file and returns them as a list.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        csv_file_path (str): The path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing a list of class names and the number of classes.\n",
        "               Returns (None, 0) if the file does not exist or an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(csv_file_path, 'r') as file:\n",
        "            reader = csv.reader(file)\n",
        "            next(reader)  # Skip header row if it exists\n",
        "            class_names = [row[2] for row in reader]  # Assuming class names are in the first column\n",
        "            class_names = sorted(set(class_names))\n",
        "        return class_names, len(class_names)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_file_path}' not found.\")\n",
        "        return None, 0\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None, 0"
      ],
      "metadata": {
        "id": "0czv7aeM3wTU"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n"
      ],
      "metadata": {
        "id": "hqeXHcy9CIFQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and normalization\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "def fix_channel(matrix):\n",
        "  return matrix.repeat(3, 1, 1) # 1s are due to height and weight\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    #transforms.Grayscale(channel = 3), #cause im getting the error message\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(fix_channel),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset classes: {train_loader.dataset.classes}\")\n",
        "print(f\"Test dataset classes: {test_loader.dataset.classes}\")\n",
        "\n",
        "splitter_web = {\n",
        "    #something gets in here\n",
        "}\n",
        "\n",
        "taser_test = {}\n",
        "grenade_train = {}\n",
        "\n",
        "\n",
        "total_class_train = {}\n",
        "total_class_test = {}\n",
        "\n",
        "for image, label in train_dataset:\n",
        "  if label in total_class_train:\n",
        "      total_class_train[label] += 1\n",
        "\n",
        "  else:\n",
        "      total_class_train[label] = 1\n",
        "\n",
        "  if label not in splitter_web:\n",
        "      splitter_web[label] = []\n",
        "  splitter_web[label].append(image)\n",
        "\n",
        "\n",
        "print(f\"Trained class {total_class_train}\")\n",
        "\n",
        "for image, label in test_dataset:\n",
        "  if label in total_class_test:\n",
        "      total_class_test[label] += 1\n",
        "\n",
        "  else:\n",
        "      total_class_test[label] = 1\n",
        "\n",
        "  if label not in splitter_web:\n",
        "      splitter_web[label] = []\n",
        "  splitter_web[label].append(image)\n",
        "\n",
        "#THHHHHE SPLLLLIIIIITTT\n",
        "for label, imagelist in splitter_web.items():\n",
        "  grenade_trainer, taser_tester = train_test_split(imagelist, test_size = 0.5, random_state = 42)\n",
        "  taser_test[label] = taser_tester\n",
        "  grenade_train[label] = grenade_trainer\n",
        "\n",
        "print(f\"Tested class {total_class_test}\")\n",
        "\n",
        "#If i called the confusion matrix and the rest here it will work\n",
        "\n"
      ],
      "metadata": {
        "id": "u8SYV_DN4HLk",
        "outputId": "9c499968-1f0e-462e-d2d0-e90e12e64d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
            "Test dataset classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
            "Trained class {5: 5421, 0: 5923, 4: 5842, 1: 6742, 9: 5949, 2: 5958, 3: 6131, 6: 5918, 7: 6265, 8: 5851}\n",
            "Tested class {7: 1028, 2: 1032, 1: 1135, 0: 980, 4: 982, 9: 1009, 5: 892, 6: 958, 3: 1010, 8: 974}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bSLMJeoIgInl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify that all folders are accessible from mounted Google Drive\n",
        "data_dir = '/content/drive/MyDrive/Datasets/GroceryStoreDataset'\n",
        "for x in ['train', 'test']:\n",
        "  path_new = os.path.join(data_dir, x)\n",
        "  print(path_new)\n",
        "\n",
        "  if (os.path.exists(path_new)):\n",
        "    for folder in os.listdir(path_new):\n",
        "      folder_path = os.path.join(path_new, folder)\n",
        "      print(folder_path)\n",
        "  else:\n",
        "    print(f\"doesn't work I'm afraid\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzWojEfRI_mG",
        "outputId": "c24fc8ab-6e88-4807-bd46-5ea1227474bf"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Datasets/GroceryStoreDataset/train\n",
            "doesn't work I'm afraid\n",
            "/content/drive/MyDrive/Datasets/GroceryStoreDataset/test\n",
            "doesn't work I'm afraid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'test']}  # Assuming you have train and test folders\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n",
        "\n",
        "print(f\"Dataset sizes: {dataset_sizes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "QxlSKZia4Nnq",
        "outputId": "674116f6-3f93-4406-e0f5-aa3c3e6c1615"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/GroceryStoreDataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-7106fafa23a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n\u001b[0m\u001b[1;32m      2\u001b[0m                   for x in ['train', 'test']}  # Assuming you have train and test folders\n\u001b[1;32m      3\u001b[0m dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2)\n\u001b[1;32m      4\u001b[0m               for x in ['train', 'test']}\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-7106fafa23a5>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n\u001b[0m\u001b[1;32m      2\u001b[0m                   for x in ['train', 'test']}  # Assuming you have train and test folders\n\u001b[1;32m      3\u001b[0m dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2)\n\u001b[1;32m      4\u001b[0m               for x in ['train', 'test']}\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Datasets/GroceryStoreDataset/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))\n",
        "\n",
        "class_names = image_datasets['test'].classes\n",
        "\n",
        "print(class_names)\n",
        "print(len(class_names))\n",
        "\n",
        "num_classes = len(class_names)"
      ],
      "metadata": {
        "id": "dQfGgMDJJzbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained EfficientNetB4\n",
        "model = models.efficientnet_b4(pretrained=True)\n",
        "\n",
        "# Modify the classifier\n",
        "num_ftrs = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "-HTLL6AAcz85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "dea2e7db-9246-4c56-c5c1-aacc14dd01dd"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B4_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B4_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n",
            "100%|██████████| 74.5M/74.5M [00:01<00:00, 72.5MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_classes' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-63de13782e9f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Modify the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_classes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(image_path, model, class_names):\n",
        "  img = Image.open(image_path).convert('RGB')\n",
        "  img_t = data_transforms['test'](img).unsqueeze(0)\n",
        "  img_t = img_t.to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    out = model(img_t)\n",
        "    _, index = torch.max(out, 1)\n",
        "    percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\n",
        "    print(f\"Predicted Class: {class_names[index[0]]}, Confidence: {percentage[index[0]].item():.2f}%\")"
      ],
      "metadata": {
        "id": "xiQz7UkGmp-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, num_epochs=25):\n",
        "    history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        now = datetime.now()\n",
        "        print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            i = 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                i += 1\n",
        "                if i % 10 == 0:\n",
        "                    print(f\"Batch {i} of {len(dataloaders[phase])}\")\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            history[f'{phase}_loss'].append(epoch_loss)\n",
        "            history[f'{phase}_acc'].append(epoch_acc)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def result():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.to(device)\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "\n",
        "          _, predicted = torch.max(output.data, 1)\n",
        "          total += target.size(0)\n",
        "          correct += (predicted == target).sum().item()\n",
        "          all_preds.extend(predicted.cpu().numpy())\n",
        "          all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "\n",
        "    #to make it more easier to read for myself\n",
        "    myacc = correct / total\n",
        "    myfone = f1_score(all_targets, all_preds, average=None)\n",
        "    myprec = precision_score(all_targets, all_preds, average=None)\n",
        "    myrecall = recall_score(all_targets, all_preds, average=None)\n",
        "    mymat = confusion_matrix(all_targets, all_preds)\n",
        "    return myacc, myfone, myprec, myrecall, mymat\n",
        "\n",
        "\n",
        "\n",
        "donemodel, donehistory = train_model(model, criterion, optimizer, num_epochs=25)\n",
        "print(f\"The model: {donemodel}\")\n",
        "print(f\"The history: {donehistory}\")\n",
        "\n",
        "\n",
        "\n",
        "doneacc, donefone, doneprec, donerecall, donemat = result()\n",
        "print(f\"The Accuracy: {doneacc}\")\n",
        "print(f\"The F1 score: {donefone}\")\n",
        "print(f\"The Precision score: {doneprec}\")\n",
        "print(f\"The Recall score: {donerecall}\")\n",
        "print(f\"The Confusion matrix: {donemat}\")\n",
        "\n",
        "##time for the 6x4 thing. lots or resources spent reviewring and studying lol\n",
        "model.eval()\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "    for data, label in test_loader:\n",
        "      output = model(data)\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      grid, axes = plt.subplots(6, 4, figsize = (12, 18))\n",
        "\n",
        "      axes = axes.flatten()\n",
        "\n",
        "      for i in range(24):\n",
        "        axes[i].imshow(data[i].cpu().permute(1, 2, 0).numpy())\n",
        "        axes[i].set_title(f\"Real {label[i].item()} vs predicted {predicted[i].item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "uYHty5B3c0tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before training\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Mango/Mango_002.jpg', model, class_names)\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Pineapple/Pineapple_021.jpg', model, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "_JS0IIf3nomp",
        "outputId": "a9d33347-8679-4783-896e-953cb123308f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-08539ef0cfa6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Mango/Mango_002.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredict_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Pineapple/Pineapple_021.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = train_model(model, criterion, optimizer, num_epochs=18)\n"
      ],
      "metadata": {
        "id": "XmK6M7JfdI6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "896253fe-1e5e-43c4-82d1-66f0312cac4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'criterion' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-581b3ea00bce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'criterion' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'train_loss': history['train_loss'],\n",
        "    'train_acc': [t.cpu().item() for t in history['train_acc']],\n",
        "    'test_loss': history['test_loss'],\n",
        "    'test_acc': [t.cpu().item() for t in history['test_acc']]\n",
        "}\n",
        "\n",
        "print(metrics)\n",
        "\n",
        "# Plot the training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(metrics['train_loss'], label='Train Loss')\n",
        "plt.plot(metrics['test_loss'], label='Test Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(metrics['train_acc'], label='Train Accuracy')\n",
        "plt.plot(metrics['test_acc'], label='Test Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "t16NKwwMk8Ll",
        "outputId": "6ca7df61-f24f-4c31-dace-a8d18ab7a91c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0e21c9682dec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m metrics = {\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the trained model\n",
        "torch.save(model, '/content/drive/MyDrive/Datasets/GroceryStoreDataset/complete_model.pth')\n"
      ],
      "metadata": {
        "id": "UIVE84gMkWdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Mango/Mango_002.jpg', model, class_names)\n",
        "predict_image('/content/drive/MyDrive/Datasets/GroceryStoreDataset/test/Pineapple/Pineapple_021.jpg', model, class_names)\n"
      ],
      "metadata": {
        "id": "dK1rXYLzdRf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Project:\n",
        "\n",
        "1. Perform exploratory data analysis on the 'train' and 'test' datasets to calculate class imbalance (by comparing 'samples per class' across all the classes)\n",
        "2. Print confusion matrix, precision, recall and f1-score\n",
        "3. Show a grid of 6x4 images, with actual and predicted class for each of those\n",
        "\n",
        "\n",
        "### Bonus Project:\n",
        "\n",
        "1. Allow user to input the items they shopped using images, use model to identify grocery item based on confidence threshold. If confidence is low, ask user to manually input the item.\n",
        "2. Update the digital grocery cart\n",
        "3. Process the transaction by generating a transaction receipt\n"
      ],
      "metadata": {
        "id": "LTcyENYCtxXm"
      }
    }
  ]
}