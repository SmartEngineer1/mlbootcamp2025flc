{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SmartEngineer1/mlbootcamp2025flc/blob/main/8_DeepLearning_ClassificationTaskMohsinMohammad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning"
      ],
      "metadata": {
        "id": "-Z0nY1bPcy7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Neural Networks\n",
        "\n",
        "- Traditional Machine Learning algorithms were trained on extracted features.\n",
        "  - Scaled well for problems involving structured data (tabular)\n",
        "  - Did not scale well for problems involving unstructured data (images, videos, audio, speech, etc)\n",
        "\n",
        "Additional References:\n",
        "1. CMU: 11-785 Introduction to Deep Learning (Spring 2025)\n",
        "https://youtu.be/NXYrIEP1LRs"
      ],
      "metadata": {
        "id": "TWrWzdM7dD6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "MkPubzxfvkSi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters (User Configurable)\n",
        "learning_rate = 0.001  #@param {type:\"number\"}\n",
        "random_seed = 42  #@param {type:\"integer\"}\n",
        "num_epochs = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ],
      "metadata": {
        "id": "fk6gFDOYvo_L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Device: {device}\")"
      ],
      "metadata": {
        "id": "epiGCioGvz-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f036411-3b3a-4aa8-af4f-d7319fb7b135"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])"
      ],
      "metadata": {
        "id": "k9O1SvBQv-b3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "print(f\"Train dataset classes: {train_loader.dataset.classes}\")\n",
        "print(f\"Test dataset classes: {test_loader.dataset.classes}\")"
      ],
      "metadata": {
        "id": "OQw8QnFqwDC0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0384a4de-feb8-4936-e4f2-dc909b430263"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 41.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.18MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.20MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.03MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
            "Test dataset classes: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YpUZqTNIwgwd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "L2hkNF8tw6Hp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    all_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # print(f\"Epoch = {epoch}\")\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # print(f\"batch number = {batch_idx}\")\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        all_loss += loss.item()\n",
        "    return correct / total, all_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "CKeLHbmfw8iU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop and generating evaluation metrics\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          _, predicted = torch.max(output.data, 1)\n",
        "          total += target.size(0)\n",
        "          correct += (predicted == target).sum().item()\n",
        "          loss = criterion(output, target)\n",
        "          all_loss += loss.item()\n",
        "    return correct / total, all_loss / len(test_loader)"
      ],
      "metadata": {
        "id": "058m--EZw-x0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test accuracy before training\n",
        "accuracy, theloss = test()\n",
        "print(f'Before training: Test Accuracy {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "id": "_Id_B7XFxWj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76602c45-8010-4d0f-d03e-99a735129eac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training: Test Accuracy 10.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    #train(epoch)\n",
        "    accuracies, losses = test()\n",
        "    print(f'Epoch {epoch}: Test Accuracy {accuracies * 100:.2f}%')\n",
        "    print(f'Epoch {epoch}: Test Loss {losses * 100:.2f}%')"
      ],
      "metadata": {
        "id": "iHUwBuzExBk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c08cfa-c1e9-494d-844d-d1a08a879a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Test Accuracy 95.22%\n",
            "Epoch 1: Test Loss 16.31%\n",
            "Epoch 2: Test Accuracy 95.22%\n",
            "Epoch 2: Test Loss 16.31%\n",
            "Epoch 3: Test Accuracy 95.22%\n",
            "Epoch 3: Test Loss 16.31%\n",
            "Epoch 4: Test Accuracy 95.22%\n",
            "Epoch 4: Test Loss 16.31%\n",
            "Epoch 5: Test Accuracy 95.22%\n",
            "Epoch 5: Test Loss 16.31%\n",
            "Epoch 6: Test Accuracy 95.22%\n",
            "Epoch 6: Test Loss 16.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional Evaluations (after training)\n",
        "print(classification_report(targets, predictions))\n",
        "\n",
        "cm = confusion_matrix(targets, predictions)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "      total += target.size(0)\n",
        "      correct += (predicted == target).sum().item()\n",
        "      all_preds.extend(predicted.cpu().numpy())\n",
        "      all_targets.extend(target.cpu().numpy())\n",
        "      for i in range(3):\n",
        "        print(f'predicated {all_preds} vs real {all_targets}')"
      ],
      "metadata": {
        "id": "9W3NhCA0xFfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXERCISE:\n",
        "1. Modify the code to plot the training accuracy and loss over the training epochs\n",
        "\n",
        "2. Modify the code to plot the test accuracy over the training epochs\n",
        "\n",
        "3. Plot example inferences and show actual and predicted values"
      ],
      "metadata": {
        "id": "hU4zHIkux7Fx"
      }
    }
  ]
}